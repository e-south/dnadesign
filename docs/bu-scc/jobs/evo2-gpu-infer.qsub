#!/bin/bash -l
#$ -N dnadesign_evo2_gpu
#$ -l h_rt=04:00:00
#$ -pe omp 4
#$ -l mem_per_core=8G
#$ -l gpus=1
#$ -l gpu_c=8.9
#$ -j y
#$ -o outputs/logs/$JOB_NAME.$JOB_ID.out

set -euo pipefail

# BU SCC GPU runtime/resource policies apply; verify current limits in SCC docs.
# Scheduler reference: https://www.bu.edu/tech/support/research/system-usage/running-jobs/submitting-jobs/

module purge
module avail cuda
module avail gcc

if [[ -n "${CUDA_MODULE:-}" ]]; then
  module load "$CUDA_MODULE"
else
  echo "CUDA_MODULE not set; using currently loaded CUDA environment."
fi

if [[ -n "${GCC_MODULE:-}" ]]; then
  module load "$GCC_MODULE"
else
  echo "GCC_MODULE not set; using currently loaded GCC environment."
fi

export CC="$(which gcc)"
export CXX="$(which g++)"
export CUDAHOSTCXX="$(which g++)"

# Replace with your package-specific GPU workload command.
uv run python - <<'PY'
import torch
print('cuda available:', torch.cuda.is_available())
if torch.cuda.is_available():
    print('device:', torch.cuda.get_device_name(0), 'cc:', torch.cuda.get_device_capability(0))
PY
